# 工作框架

1. PCIE的配置空间初始化：PCIE卡都遵循一个标准, x86通过往2个内存地址读写就可以控制IO桥访问一个内部寄存器+一个地址偏移, 就可以读写PCI的配置空间, 操作系统实际上就是用这个机制, 判断卡位是否插上了卡, 卡是否合法, 以及写对应的配置区域(相当于初始化)；
2. msix机制及初始化：OS在初始化配置区的时候, 会根据卡将pci卡的msix起始地址写到pci配置的扩展能力区域, 驱动只需要去读取对应的区域, 像os申请msix向量, 即可使用msix中断是一种特殊的中断, 不需要中断线, 但需要PCIE具备msix能力, 主机也必须支持apic才可. 当系统初始化时, 同时初始化主机上2个特殊硬件, IOAPIC和LocalAPIC, 在内存虚拟地址中开辟一段内存, 给每个cpu分配中断向量. 后面只要往这个内存上写触发设备信息, 那么就会被内存控制器劫持, 内存控制立即明白这是有外设触发了中断, 通知ioapic发送广播, 当对应的cpu判断对应的向量, 知道这个是要被自己处理, 就会处理这个中断。
3. napi机制：napi也是网络设备的一个机制, 把设备的napi的list挂到系统上, 随即发送一个软中断, 调用一个回调函数。
4. dma机制：e1000采用的是自动收发, 就是说数据包从网卡的fifo到skb里面, 或者从skb到网卡的fifo是由dma自动完成的, 在完成后会触发msix中断

## 工作原理

## 数据结构

### NAPI相关数据结构

- softnet_data
- 

### GRO相关数据结构

## 初始化

`net_dev_init()`入口

1. `dev_proc_init`注册 /proc/net/{dev,softnet_stat,ptytpe}
2. `netdev_kobject_init()`: 内核对象初始化
3. 初始化全局变量`ptype_base[PTYPE_HASH_SIZE]`
4. 针对每个 CPU，初始化各种接收队列（packet receive queues）
   1. flush_works
   2. softnet_data: sd->input_pkt_queue, sd->process_queue,sd->xfrm_backlog, sd->poll_list
5. RPS(receive packet steering)
6. GRO: sd->backlog
7. register_pernet_device(&loopback_net_ops)
8. 软中断处理：RX、TX
9. cpuhp_setup_state_nocalls： cpu事件，dev_cpu_dead
10. NAPI: 从ring buffer到socket bufffer。

## 收包过程

- net_rx_action(struct softirq_action *h)： 软中断函数，从CPU的softnet_data数据结构判断启动哪些网卡的napi_poll函数，对于不支持NAPI的网卡驱动模拟一个驱动处理。软中断处理过程中会限制处理的数据包数量和执行时间，避免调用过大的调用导致cpu响应其他应用延迟。
- static int napi_poll(struct napi_struct *n, struct list_head *repoll): 调用对应网卡(napi)的poll函数,返回处理了多少个数据包。这个函数将在`napi_struct.weight`规定的时间内，被`net_rx_action`循环调用，直到时间片用尽或者网卡当前DMA中所有缓存的数据包被处理完。如果是由于时间片用尽而退出的的话，`napi_struct`会重新挂载到`softnet_data`上，而如果是所有数据包处理完退出的，`napi_struct`会从`softnet_data`上移除并重新打开网卡硬件中断。
- int __napi_poll(struct napi_struct *n, bool *repoll)： NAPI的状态变化，调用napi->poll()函数。1. 如果driver.poll()用完了weight，不修改NAPI的状态，由net_rx_action() 变更。 一次poll干不完活，再干不干有软中断看看自己的预算。2. 如果driver.poll()没用完weight，必须关闭NAPI。下次硬件中断，网卡中断处理函数再次调用napi_schedule(napi), 再次处理。很容易理解，一次poll干完活了，就等待硬中断再次启动poll。 
- e1000_clean
- e1000_clean_rx_irq
- e1000_receive_skb
- napi_gro_flush(n, HZ >= 1000)： 如果napi->gro_bitmap设置了,说明有gro包要处理， HZ>1000？
- gro_normal_list(n)： 发送gro_normal的数据包
- napi_gro_receive
- napi_skb_finish
- netif_receive_skb_internal
- __netif_receive_skb
- __netif_receive_skb_core: 将数据送到抓包点（tap）或协议层
- __netif_receive_skb_one_core
- ip_rcv
- NF_HOOK
- NF_HOOK
- ip_rcv_finish
- dst_input
- ip_local_deliver
- NF_HOOK
- NF_HOOK
- ip_local_deliver_finish
- ip_protocol_deliver_rcu
- tcp_v4_rcv
- tcp_child_process
- tcp_rcv_state_process
- tcp_ack

## net_rx_action() -> napi_poll()：软中断中从 ring buffer 读数据

```C
// net/core/dev.c

static __latent_entropy void net_rx_action(struct softirq_action *h) {
    struct softnet_data *sd  = this_cpu_ptr(&softnet_data);       // 该 CPU 的 softnet_data 统计
    time_limit = jiffies + usecs_to_jiffies(netdev_budget_usecs); // 该 CPU 的所有 NAPI 变量的总 time limit
    budget     = netdev_budget;                                   // 该 CPU 的所有 NAPI 变量的总预算

    LIST_HEAD(list);   // 声明 struct list_head list 变量并初始化;
    LIST_HEAD(repoll); // 声明 struct list_head repoll 变量并初始化;

    local_irq_disable(); // 关闭该CPU 硬中断
    list_splice_init(&sd->poll_list, &list);
    local_irq_enable(); // 打开该CPU 硬中断

    for (;;) {
        struct napi_struct *n;
        skb_defer_free_flush(sd);
        if list_empty(&list) {  // 该CPU没有poll的任务了
            if !sd_has_rps_ipi_waiting(sd) && list_empty(&repoll)  // RPS 相关逻辑，IPI 进程间中断，其他CPU也没有任务
                goto end;
            break;
        }

        n = list_first_entry(&list, struct napi_struct, poll_list);
        budget -= napi_poll(n, &repoll); // 执行网卡驱动注册的 poll() 方法，返回的是处理的数据帧数量，
                                         // 函数返回时，那些数据帧都已经发送到上层栈进行处理了。

        if (budget <= 0 || time_after_eq(jiffies, time_limit)) { // budget 或 time limit 用完了
            sd->time_squeeze++;                                  // 更新 softnet_data.time_squeeze 计数
            break;
        }
    }

    local_irq_disable();
    list_splice_tail_init(&sd->poll_list, &list);
    list_splice_tail(&repoll, &list);
    list_splice(&list, &sd->poll_list);

    if !list_empty(&sd->poll_list)              // 在给定的 time/budget 内，没有能够处理完全部 napi
        __raise_softirq_irqoff(NET_RX_SOFTIRQ); // 关闭 NET_RX_SOFTIRQ 类型软中断，将 CPU 让给其他任务用，
                                                // 主动让出 CPU，不要让这种 softirq 独占 CPU 太久。

    // Receive Packet Steering：唤醒其他 CPU 从 ring buffer 收包。
    net_rps_action_and_irq_enable(sd);
end:;
}
```

一次poll的时间是收到限制的，避免单个网卡霸占CPU，通过处理数据帧的个数和时间进行限制一次Poll的过程。具体，有俩参数控制每次 softirq 线程的收包预算和时间：

- netdev_budget：最多可以收包的数量，注册的驱动的时候默认为300。
- netdev_budget_usecs：最长可以占用的 CPU 时间，默认为2ms。
这两个预算限制中，任何一个达到后都将导致退出本次 softirq 处理。二者的默认值：

```C
// net/core/dev.c
int netdev_budget __read_mostly = 300;
// Must be at least 2 jiffes to guarantee 1 jiffy timeout
unsigned int __read_mostly netdev_budget_usecs = 2 * USEC_PER_SEC / HZ; // 2000us
``

注意这些预算是每个 CPU 上所有 NAPI 实例共享的总预算。另外，这两个变量都是 sysctl 配置项，可以按需调优：

```shell
$ sudo sysctl -a | grep netdev_budget
net.core.netdev_budget = 300            # 300 个包
net.core.netdev_budget_usecs = 2000     # 2ms
```
###  int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc, struct packet_type **ppt_prev)

全局变量：

- static struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;
- static struct list_head ptype_all __read_mostly

几个常见的packet_type，这些都在相应的协议初始化的时候调用dev_add_pack加入到特性的链表中：

```C
static struct packet_type ip_packet_type __read_mostly = {
    .type = cpu_to_be16(ETH_P_IP),
    .func = ip_rcv,
    .gso_send_check = inet_gso_send_check,
    .gso_segment = inet_gso_segment,
    .gro_receive = inet_gro_receive,
    .gro_complete = inet_gro_complete,
};

static struct packet_type arp_packet_type __read_mostly = {
    .type = cpu_to_be16(ETH_P_ARP),
    .func = arp_rcv,
}
```

1. net_timestamp_check: 时间戳检查
2. do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb)： 进行xdp处理.如果硬件网卡不支持 XDP 程序，那 XDP 程序会推迟到这里来执行。XDP 的目的是将部分逻辑下放（offload）到网卡执行，通过硬件处理提高效率。 但是不是所有网卡都支持这个功能，所以内核引入了 Generic XDP 这样一个环境，如果网卡不支持 XDP， 那 XDP 程序就会推迟到这里来执行。它并不能提升效率，所以主要用来测试功能。更多关于 XDP 的内容： XDP (eXpress Data Path)：在操作系统内核中实现快速、可编程包处理（ACM，2018）。
3. skb_vlan_untag(skb): vlan处理，主要是循环把vlan头剥掉，如果qinq场景，两个vlan都会被剥掉；
4. deliver_skb(skb, pt_prev, orig_dev)： 遍历已注册的协议，并调用其回调函数(一般是libpcap通过`AF_PACKET`传入的)
5. sch_handle_ingress(skb, &pt_prev, &ret, orig_dev, &another);
6. vlan_do_receive(&skb)
7. 交给rx_handler处理，例如OVS、linux bridge等
8. ptype_all处理，例如抓包程序、raw socket等
9. ptype_base处理，交给协议栈处理，例如ip、arp、rarp等；
10. sch_handle_ingress() 进入 TC ingress 处理,TC（Traffic Control）是 Linux 的流量控制子系统，经典用途：限速；有了 TC BPF 之后，还能通过 BPF 编程来做流量的透明拦截和处理，例如实现 K8s 的 Service 负载均衡。
11. nf_ingress() 进入 Netfilter ingress 处理。 Netfilter 是传统的内核包过滤子系统，iptables 是其用户空间客户端。参考[深入理解 iptables 和 netfilter 架构](https://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/) 和 [连接跟踪（conntrack）：原理、应用及 Linux 内核实现](https://arthurchiao.art/blog/conntrack-design-and-implementation-zh/)。
12. skb->dev->rx_handler(&skb) 进入 L3 ingress 处理，例如 IPv4 处理逻辑
13. 如果启用了RPS，判断使用哪个 CPU 的 `backlog queue`，这个过程由 `get_rps_cpu` 函数完成。`get_rps_cpu` 会考虑前面提到的 RFS 和 aRFS 设置，以此选出一个合适的 CPU，通过调用 `enqueue_to_backlog` 将数据放到它的 `backlog queue`。
    1. enqueue_to_backlog(),首先从远端 CPU 的 struct softnet_data 获取 backlog queue 长度。如果 backlog 大于 netdev_max_backlog，或者超过了 flow limit，直接 drop，并更新 softnet_data 的 drop 统计。注意这是远端 CPU 的统计。
    2. RPS 在不同 CPU 之间分发 packet，但是，如果一个 flow 特别大，会出现单个 CPU 被打爆，而 其他 CPU 无事可做（饥饿）的状态。因此引入了 flow limit 特性，放到一个 backlog 队列的属 于同一个 flow 的包的数量不能超过一个阈值。这可以保证即使有一个很大的 flow 在大量收包 ，小 flow 也能得到及时的处理。Flow limit 功能默认是关掉的。要打开 flow limit，需要指定一个 bitmap（类似于 RPS 的 bitmap）。
    3. 处理 backlog 队列：NAPI poller,每个 CPU 都有一个 backlog queue，其加入到 NAPI 变量的方式和驱动差不多，都是注册一个 poll() 方法，在软中断的上下文中处理包。此外，还提供了一个 weight，这也和驱动类似 。

```C
static int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc, struct packet_type **ppt_prev)
{
    struct packet_type *ptype, *pt_prev;
    rx_handler_func_t *rx_handler;
    struct sk_buff *skb = *pskb;
    struct net_device *orig_dev;
    bool deliver_exact = false;
    int ret = NET_RX_DROP;
    __be16 type;

    net_timestamp_check(!READ_ONCE(netdev_tstamp_prequeue), skb); //记录收包时间，netdev_tstamp_prequeue为0，表示可能有包延迟

    trace_netif_receive_skb(skb);

    orig_dev = skb->dev; //记录收包设备

    skb_reset_network_header(skb); //重置network header，此时skb已经指向IP头（没有vlan的情况下）,//把L3、L4的头都指向data数据结构，到这里的时候skb已经处理完L2层的头了
    if (!skb_transport_header_was_set(skb))
        skb_reset_transport_header(skb);
    skb_reset_mac_len(skb); //重置mac len

    pt_prev = NULL;

another_round:
    skb->skb_iif = skb->dev->ifindex; //设置报文的iif值

    __this_cpu_inc(softnet_data.processed); //处理包数统计

    if (static_branch_unlikely(&generic_xdp_needed_key)) { // Generic XDP（软件实现 XDP 功能）
        int ret2;

        migrate_disable();
        ret2 = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);
        migrate_enable();

        if (ret2 != XDP_PASS) {
            ret = NET_RX_DROP;
            goto out;
        }
    }

    if (eth_type_vlan(skb->protocol)) { //vxlan报文处理，剥除vxlan头
        skb = skb_vlan_untag(skb); //剥除vlan头
        if (unlikely(!skb))
            goto out;
    }

    if (skb_skip_tc_classify(skb))
        goto skip_classify;

    if (pfmemalloc)  //此类报文不允许ptype_all处理，即tcpdump也抓不到
        goto skip_taps;

    list_for_each_entry_rcu(ptype, &ptype_all, list) { //遍历ptype_all，如果有则做相应处理，例如raw socket和tcpdump实现
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;    //pt_prev的加入是为了优化，只有当找到下一个匹配的时候，才执行这一次的回调函数
    }

    list_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) { //设备上注册ptype_all，做相应的处理，更加精细的控制
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;
    }

skip_taps:
#ifdef CONFIG_NET_INGRESS
    if (static_branch_unlikely(&ingress_needed_key)) { // TC ingress 处理
        bool another = false;

        nf_skip_egress(skb, true);
        skb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev, &another);
        if (another)    // TCP BPF 优化，通过 another round 将包从宿主机网卡直接送到容器 netns 内网卡
            goto another_round;
        if (!skb)
            goto out;

        nf_skip_egress(skb, false);
        if (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)  // Netfilter  ingress 处理
            goto out;
    }
#endif
    skb_reset_redirect(skb);
skip_classify:
    if (pfmemalloc && !skb_pfmemalloc_protocol(skb))
        goto drop;

    if (skb_vlan_tag_present(skb)) { // 处理 VLAN 头
        if (pt_prev) {
            ret = deliver_skb(skb, pt_prev, orig_dev);
            pt_prev = NULL;
        }
        if (vlan_do_receive(&skb)) // 根据实际的vlan设备调整信息，再走一遍
            goto another_round;
        else if (unlikely(!skb))
            goto out;
    }

    // 如果一个dev被添加到一个bridge（做为bridge的一个接口)，这个接口设备的rx_handler将被设置为br_handle_frame函数，这是在br_add_if函数中设置的，而br_add_if (net/bridge/br_if.c)是在向网桥设备上添加接口时设置的。进入br_handle_frame也就进入了bridge的逻辑代码。
    rx_handler = rcu_dereference(skb->dev->rx_handler); //设备rx_handler，加入OVS时会注册为OVS的入口函数
    if (rx_handler) {
        if (pt_prev) {
            ret = deliver_skb(skb, pt_prev, orig_dev);
            pt_prev = NULL;
        }
        switch (rx_handler(&skb)) { //执行rx_handler处理，例如进入OVS，OVS不支持报头中携带vlan的报文
        case RX_HANDLER_CONSUMED: // 已处理，无需进一步处理
            ret = NET_RX_SUCCESS;
            goto out;
        case RX_HANDLER_ANOTHER:  // 修改了skb->dev，在处理一次
            goto another_round;
        case RX_HANDLER_EXACT:   // 精确传递到ptype->dev == skb->dev
            deliver_exact = true;
            break;
        case RX_HANDLER_PASS:
            break;
        default:
            BUG();
        }
    }

    if (unlikely(skb_vlan_tag_present(skb)) && !netdev_uses_dsa(skb->dev)) {
check_vlan_id:
        if (skb_vlan_tag_get_id(skb)) {
            /* Vlan id is non 0 and vlan_do_receive() above couldn't
             * find vlan device.
             */
            skb->pkt_type = PACKET_OTHERHOST;
        } else if (eth_type_vlan(skb->protocol)) {
            /* Outer header is 802.1P with vlan 0, inner header is
             * 802.1Q or 802.1AD and vlan_do_receive() above could
             * not find vlan dev for vlan id 0.
             */
            __vlan_hwaccel_clear_tag(skb);
            skb = skb_vlan_untag(skb);
            if (unlikely(!skb))
                goto out;
            if (vlan_do_receive(&skb))
                /* After stripping off 802.1P header with vlan 0
                 * vlan dev is found for inner header.
                 */
                goto another_round;
            else if (unlikely(!skb))
                goto out;
            else
                /* We have stripped outer 802.1P vlan 0 header.
                 * But could not find vlan dev.
                 * check again for vlan id to set OTHERHOST.
                 */
                goto check_vlan_id;
        }
        /* Note: we might in the future use prio bits
         * and set skb->priority like in vlan_do_receive()
         * For the time being, just ignore Priority Code Point
         */
        __vlan_hwaccel_clear_tag(skb);
    }

    type = skb->protocol;

    /* deliver only exact match when indicated */
    if (likely(!deliver_exact)) { 
        deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type, //根据全局定义的协议处理报文
                       &ptype_base[ntohs(type) &
                           PTYPE_HASH_MASK]);
    }

    deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type, //根据设备上注册的协议进行处理
                   &orig_dev->ptype_specific);

    if (unlikely(skb->dev != orig_dev)) {
        deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,
                       &skb->dev->ptype_specific); //如果设备发生变化，那么还需要针对新设备的注册协议进行处理
    }

    if (pt_prev) {
        if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
            goto drop;
        *ppt_prev = pt_prev;
    } else {
drop:
        if (!deliver_exact)
            dev_core_stats_rx_dropped_inc(skb->dev);
        else
            dev_core_stats_rx_nohandler_inc(skb->dev);
        kfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);
        /* Jamal, now you will not able to escape explaining
         * me how you were going to use this. :-)
         */
        ret = NET_RX_DROP;
    }

out:
    /* The invariant here is that if *ppt_prev is not NULL
     * then skb should also be non-NULL.
     *
     * Apparently *ppt_prev assignment above holds this invariant due to
     * skb dereferencing near it.
     */
    *pskb = skb;
    return ret;
}
```


### softnet_data.time_squeeze: ring buffer 还有包，预算用完了

## napi_poll(napi) -> driver.poll(napi, weight): 网卡poll

根据napi设计的目标： 

1. 如果driver.poll()用完了weight，不修改NAPI的状态，有net_rx_action() 变更。 一次poll干不完活，再干不干有软中断看看自己的预算。
2. 如果driver.poll()没用完weight，必须关闭NAPI。下次硬件中断，网卡中断处理函数再次调用napi_schedule(napi), 再次处理。很容易理解，一次poll干完活了，就等待硬中断再次启动poll。 

## dev_weight: 网卡一次poll，最多处理的包数

```C
    netif_napi_add(netdev, &c->napi, mlx5e_napi_poll, 64);
```

```shell
$ sudo sysctl -a | grep dev_weight
net.core.dev_weight = 64
net.core.dev_weight_rx_bias = 1
net.core.dev_weight_tx_bias = 1
```

## GRO(Generic Receive Offloading)

LRO(Large Receive Offloading)是一种比较老的硬件特性，GRO是LRO的软件实现。GRO通过合并足够类似的包，保留一个header，合并数据，对其进行检查校验和、修改协议头、发送应答包，再交给网络协议层，减少CPU的使用量。

1. 降低CPU的使用量，协议层只处理一个skb。
2. 信息丢失：包的option和flag信息在合并时丢失
3. 合并规则：
   1. GRO的buffer相对于包太小，不合并
   2. 当前包属于更大包的分片，使用`enqueue_backlog()`将分片放到cpu的包队列中，重组后交给协议栈。
   3. 当前包不是分片包，直接传递协议层。

merge的情况：

1. GRO_NORMAL //返回值为normal，则直接提交报文给协议栈, netif_receive_skb(skb)
2. GRO_DROP
3. GRO_MERGED_FREE //报文已经merge，需要释放skb
4. GRO_HELD   // 这个表示当前数据已经被gro保存起来，但是并没有进行合并，因此skb还需要保存。
5. GRO_MERGED //报文已经被保存到gro_list中，不要求释放sk

主要是利用Scatter-gather IO，也就是skb的struct skb_shared_info域。
每个NAPI的实例都会包括一个域叫gro_list,保存了我们积攒的数据包(将要被merge的).然后每次进来的skb都会在这个链表里面进行查找，看是否需要merge。而gro_count表示当前的gro_list中的skb的个数。
每一层协议都实现了自己的gro回调函数，gro_receive和gro_complete，gro系统会根据协议来调用对应回调函数，其中gro_receive是将输入skb尽量合并到我们gro_list中。而gro_complete则是当我们需要提交gro合并的数据包到协议栈时被调用的。

### 数据结构

### 处理过程


napi_skb_finish(dev_gro_receive(napi, skb), skb);    //gro收包并提交给协议栈处理，dev_gro_receive函数的返回值决定如何处理报文



### napi_gro_receive()

将skb包含的gro上下文reset，然后调用__napi_gro_receive,最终通过napi_skb_finis来判断是否需要讲数据包feed进协议栈。

### dev_gro_receive(napi, skb)

遍历gro_list判断是否应该gro处理的数据包：

1. 首先网卡支持GRO，skb->dev->features & NETIF_F_GRO
2. skb_is_gso(skb) || skb_has_frag_list(skb) || skb->csum_bad)    //如果报文是GSO报文 或者 包含frag_list，或csum_bad则提交给协议栈处理
3. 遍历gro_list中的报文和当前报文是否同流： 相同的入口设备、vlan_tci、mac头相同
4. //遍历packet_offload链表，找到和当前协议相同的packet_offload，IP报文为ip_packet_offload
   1. //设置network header，驱动调用napi_gro_receive前需要把报文移到network header
   2. //根据ip_summed字段初始化参数
   3. //调用网络层的gro_receive函数
5. //没有匹配到packet_offload对象，则直接提交报文给协议栈
6. //网络层gro_receive处理后，same_flow可能被刷新，//如果是相同的流，则返回GRO_MERGED_FREE 或 GRO_MERGED，报文不会被提交给协议栈
7. //未匹配到流，且flush被置1，则直接提交报文给协议栈
8. //未匹配到流，且flush未被置1，则把该报文插入到gro_list中，待以后匹配处理




### napi_ksb_finish(napi, skb, ret)

主要是通过判断传递进来的ret(__napi_gro_receive的返回值),来决定是否需要feed数据进协议栈。它的第二个参数是前面处理过的skb。

### gro_normal_one(napi, skb)

## 创建内核skb数据包

### L2层数据处理

1. l2 header
2. ipsec
3. 时间戳

## 重新填充RX ring

## RSS (Receive Side Scaling)：多队列分发，接收端水平扩展

1. 网卡硬件支持多队列。
2. 网卡驱动通过dma映射到多个内存区域。
3. 多个napi管理软中断的poll过程。

## RPS (Receive Packet Steering)：RSS 的软件实现

对于不支持RSS的网卡通过软件方式，实现多个CPU处理，从其他CPU进入协议栈。

1. package通过DMA进入内存后，RPS才可以能工作。
2. RPS不会减少CPU处理硬中断和NAPI poll()软中断的时间。
3. 对接受到的package做hash，以确定使用哪个CPU处理，将package放到每个CPU独占的backlog队列。
4. 当前cpu向目标cpu发送IPI进程间通信，会触发目标端cpu处理backlog队列收包。 可以在`cat /proc/net/softnet_stat` 的第 10 列。

## RFS (Receive Flow Steering)：相同 flow 的包送到同一 CPU

RFS结合RPS使用，将相同flow的包发送到相同的CPU处理，提高缓存命中率。

## aRFS (Hardware accelerated RFS)

如果网卡驱动实现了 `ndo_rx_flow_steer()` 方法，可以利用网卡驱动判断应该放在哪个CPU商场婚礼，支持 RFS。

## xdp

## Netpoll: 内核挂掉后，设备仍然能收发数据

Linux 内核提供了一种方式，在内核挂掉（crash）时，设备驱动仍然可以接收和发送数据， 相应的 API 被称作 Netpoll。这个功能在一些特殊的网络场景有用途，两个著名的例子：kgdb, netconsole。

netconsole 的说明：

This module logs kernel printk messages over UDP allowing debugging of problem where disk logging fails and serial consoles are impractical.It can be used either built-in or as a module. As a built-in, netconsole initializes immediately after NIC cards and will bring up the specified interface as soon as possible. While this doesn’t allow capture of early kernel panics, it does capture most of the boot process.

大部分驱动都支持 Netpoll 功能。支持此功能的驱动需要实现 struct net_device_ops 的 ndo_poll_controller 方法。

想进一步了解其使用方式，可在内核代码中 grep -R netpoll_ net/*。